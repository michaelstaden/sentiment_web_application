{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>+++ Project still in progres +++<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will use **Natural Language Processing (NLP)** techniques to analyze the twitter dataset **sentiment140**. The dataset consists of 1.6 million tweets, which are labeled as positive, neutral or negative regarding their sentiment. \n",
    "\n",
    "In the first part data preprocessing is conducted. In the second part the model is built using **Logistic Regression** and gets evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T10:57:37.645082Z",
     "start_time": "2020-09-09T10:57:37.543673Z"
    }
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import ticker\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T10:57:48.345757Z",
     "start_time": "2020-09-09T10:57:37.651246Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "columns = ['target', 'id', 'date', 'query_string', 'user', 'text']\n",
    "df = pd.read_csv('trainingdata.csv', header=None, names=columns, encoding='latin1')\n",
    "df = df[['target', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T10:57:48.384050Z",
     "start_time": "2020-09-09T10:57:48.351395Z"
    }
   },
   "outputs": [],
   "source": [
    "decode_map = {0: 'negative', 2: 'neutral', 4: 'positive'}\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T10:57:51.117167Z",
     "start_time": "2020-09-09T10:57:48.415162Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "df.target = df.target.apply(lambda x: decode_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T10:57:51.178858Z",
     "start_time": "2020-09-09T10:57:51.122450Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T10:57:51.757064Z",
     "start_time": "2020-09-09T10:57:51.184677Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(['target']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T10:57:52.630904Z",
     "start_time": "2020-09-09T10:57:51.762564Z"
    }
   },
   "outputs": [],
   "source": [
    "target_cnt = Counter(df.target)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(target_cnt.keys(), target_cnt.values())\n",
    "plt.title('Dataset labels distribution', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.569Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectorizer.fit(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.573Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "#Create matrices with word counts for neg and pos texts\n",
    "neg_doc_matrix = vectorizer.transform(df[df.target == 'negative'].text)\n",
    "pos_doc_matrix = vectorizer.transform(df[df.target == 'positive'].text)\n",
    "\n",
    "#Create 2 lists with counts for each word in all neg and pos texts\n",
    "neg_tf = np.sum(neg_doc_matrix, axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix, axis=0)\n",
    "\n",
    "#Convert neg and pos word count lists from from dim (1, 684358) to dim (684358,)\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "\n",
    "#Combine two arrays from neg and pos word counts to dataframe and add words as headers\n",
    "term_freq_df = pd.DataFrame([neg,pos], columns=vectorizer.get_feature_names()).transpose()\n",
    "\n",
    "term_freq_df.columns = ['negative', 'positive']\n",
    "term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['positive']\n",
    "\n",
    "term_freq_df.sort_values(by='total', ascending=False).iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just out of curiosity let's have a look at the most frequent negative and positive words **before the preprocessing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.577Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pos = np.arange(50)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.barh(y_pos, term_freq_df.sort_values(by='negative', ascending=False)['negative'][:50], align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, term_freq_df.sort_values(by='negative', ascending=False)['negative'][:50].index, rotation='horizontal')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Top 50 negative tokens')\n",
    "plt.title('Top 50 tokens in negative tweets before preprocessing', fontsize=18)\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.581Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pos = np.arange(50)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.barh(y_pos, term_freq_df.sort_values(by='positive', ascending=False)['positive'][:50], align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, term_freq_df.sort_values(by='positive', ascending=False)['positive'][:50].index, rotation='horizontal')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Top 50 positive tokens')\n",
    "plt.title('Top 50 tokens in positive tweets before preprocessing')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's conduct the preprocessing and have again a look at the most frequent words in positive and negative tweets and compare the results to the bar charts before the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.586Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "## Defining set containing all stopwords in english.\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.591Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    \n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    \n",
    "    # Defining regex patterns.\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    \n",
    "    #for tweet in textdata:\n",
    "        #tweet = tweet.lower()\n",
    "    tweet = textdata.lower()\n",
    "    # Replace all URls with 'URL'\n",
    "    tweet = re.sub(urlPattern,' ',tweet)#re.sub(urlPattern,' URL',tweet)\n",
    "    # Replace all emojis.\n",
    "    for emoji in emojis.keys():\n",
    "        tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
    "    # Replace @USERNAME to 'USER'.\n",
    "    tweet = re.sub(userPattern,' ', tweet)#tweet = re.sub(userPattern,' USER', tweet)         \n",
    "    # Replace all non alphabets.\n",
    "    tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "    # Replace 3 or more consecutive letters by 2 letter.\n",
    "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "    tweetwords = ''\n",
    "    for word in tweet.split():\n",
    "        # Checking if the word is a stopword.\n",
    "        if word not in stopwordlist:\n",
    "            if len(word)>1:\n",
    "                # Lemmatizing the word.\n",
    "                word = wordLemm.lemmatize(word)\n",
    "                tweetwords += (word+' ')\n",
    "            \n",
    "        # processedText.append(tweetwords)\n",
    "        \n",
    "    return tweetwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.595Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "df.text = df.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.603Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "vectorizer_prep = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectorizer_prep.fit(df.text)\n",
    "print('No. of feature_words: ', len(vectorizer_prep.get_feature_names()))\n",
    "print(vectorizer_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.609Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "#Create matrices with word counts for neg and pos texts\n",
    "neg_doc_matrix_prep = vectorizer_prep.transform(df[df.target == 'negative'].text)\n",
    "pos_doc_matrix_prep = vectorizer_prep.transform(df[df.target == 'positive'].text)\n",
    "\n",
    "#Create 2 lists with counts for each word in all neg and pos texts\n",
    "neg_tf_prep = np.sum(neg_doc_matrix_prep, axis=0)\n",
    "pos_tf_prep = np.sum(pos_doc_matrix_prep, axis=0)\n",
    "\n",
    "#Convert neg and pos word count lists from from dim (1, 684358) to dim (684358,)\n",
    "neg_prep = np.squeeze(np.asarray(neg_tf_prep))\n",
    "pos_prep = np.squeeze(np.asarray(pos_tf_prep))\n",
    "\n",
    "#Combine two arrays from neg and pos word counts to dataframe and add words as headers\n",
    "term_freq_df_prep = pd.DataFrame([neg_prep,pos_prep], columns=vectorizer_prep.get_feature_names()).transpose()\n",
    "\n",
    "term_freq_df_prep.columns = ['negative', 'positive']\n",
    "term_freq_df_prep['total'] = term_freq_df_prep['negative'] + term_freq_df_prep['positive']\n",
    "\n",
    "term_freq_df_prep.sort_values(by='total', ascending=False).iloc[0:10];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.614Z"
    }
   },
   "outputs": [],
   "source": [
    "x_range = np.arange(50)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.barh(x_range, term_freq_df_prep.sort_values(by='negative', ascending=False)['negative'][:50], align='center', alpha=0.5)\n",
    "plt.yticks(x_range, term_freq_df_prep.sort_values(by='negative', ascending=False)['negative'][:50].index, rotation='horizontal')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Top 50 negative tokens')\n",
    "plt.title('Top 50 tokens in negative tweets after preprocessing', fontsize=18)\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.621Z"
    }
   },
   "outputs": [],
   "source": [
    "x_range = np.arange(50)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.barh(x_range, term_freq_df_prep.sort_values(by='positive', ascending=False)['positive'][:50], align='center', alpha=0.5)\n",
    "plt.yticks(x_range, term_freq_df_prep.sort_values(by='positive', ascending=False)['positive'][:50].index, rotation='horizontal')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Top 50 positive tokens')\n",
    "plt.title('Top 50 tokens in positive tweets after preprocessing', fontsize=18)\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.626Z"
    }
   },
   "outputs": [],
   "source": [
    "mask = np.array(Image.open('positive.PNG'))\n",
    "plt.figure(figsize=(12, 6))\n",
    "wc = WordCloud(mask=mask, background_color='white', max_words=2000, width=mask.shape[1], height=mask.shape[0]).generate(' '.join(df[df['target']=='positive'].iloc[0:200000].text))\n",
    "plt.axis('off')\n",
    "plt.title('p as in positive', fontsize=18)\n",
    "plt.imshow(wc, interpolation='bilinear');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.629Z"
    }
   },
   "outputs": [],
   "source": [
    "mask = np.array(Image.open('negative.PNG'))\n",
    "plt.figure(figsize=(12, 6))\n",
    "wc = WordCloud(mask=mask, background_color='white', max_words=2000, width=mask.shape[1], height=mask.shape[0]).generate(' '.join(df[df['target']=='negative'].iloc[0:200000].text))\n",
    "plt.axis('off')\n",
    "plt.title('n as in negative', fontsize=18)\n",
    "plt.imshow(wc, interpolation='bilinear');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelling, Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section a **Logistic Regression** classifier is trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.636Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.640Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "vectorizer_prep = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectorizer_prep.fit(X_train)\n",
    "\n",
    "print('No. of feature_words: ', len(vectorizer_prep.get_feature_names()))\n",
    "print(vectorizer_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.644Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer_prep.transform(X_train)\n",
    "X_test = vectorizer_prep.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.650Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    \n",
    "    # predict values for test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # print the evaluation metrics for the dataset\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # compute and plot the confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories = ['negative', 'positive']\n",
    "    group_names = ['true neg', 'false pos', 'false neg', 'true pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "    \n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    sns.heatmap(cf_matrix, annot=labels, cmap='Blues', fmt='')\n",
    "    \n",
    "    ax.set(yticks=[0, 2], xticks=[0,1], xticklabels=categories, yticklabels=categories)\n",
    "    ax.yaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0.5))\n",
    "    ax.xaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0.5))\n",
    "    plt.xlabel('Predicted values', fontdict={'size':14})\n",
    "    plt.ylabel('Actual values', fontdict={'size':14})\n",
    "    plt.title('Confusion Matrix', fontdict={'size':18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.654Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "LRmodel = LogisticRegression(C=2, solver='liblinear', max_iter=1000, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.658Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model(LRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-09T10:57:37.665Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('Sentiment-LR.pickle','wb')\n",
    "pickle.dump(LRmodel, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conlusion\n",
    "The bar charts before and after the preprocessing show the importance of these steps in NLP. The Logistic Regression classifier with accuracy rate of 81% shows good results compared to the benchmark on this dataset. Further steps could be to compare this Logistic Regression classifier to other classifiers, e.g. neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
